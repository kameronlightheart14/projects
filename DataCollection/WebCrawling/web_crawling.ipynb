{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Modify `scrape_books()` so that it gathers the price for each fiction book and\n",
    "returns the mean price, in Â£, of a fiction book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(start_page = \"index.html\"):\n",
    "    \"\"\" Crawl through http://books.toscrape.com and extract fiction data\"\"\"\n",
    "    base_url=\"http://books.toscrape.com/catalogue/category/books/fiction_10/\"\n",
    "    titles = []\n",
    "    fict_prices = []\n",
    "    page = base_url + start_page                # Complete page URL.\n",
    "    next_page_finder = re.compile(r\"next\")      # We need this button.\n",
    "    \n",
    "    current = None\n",
    "\n",
    "    for _ in range(2):\n",
    "        while current == None:                   # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.  \n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find_all(class_=\"product_pod\")\n",
    "            \n",
    "        # Navigate to the correct tag and extract title.\n",
    "        for book in current:\n",
    "            price = book.find(class_=\"product_price\").p.contents[0]\n",
    "            price = re.sub(\"[^0-9\\.]\", r\"\", price)\n",
    "            fict_prices.append(float(price))\n",
    "            titles.append(book.h3.a[\"title\"])\n",
    "    \n",
    "        # ind the URL for the page with the next data\n",
    "        if \"page-2\" not in page:\n",
    "            # Find the URL for the page with the next data.\n",
    "            new_page = soup.find(string=next_page_finder).parent[\"href\"]    \n",
    "            page = base_url + new_page      # New complete page URL.\n",
    "            current = None\n",
    "    \n",
    "    result = sum(fict_prices) / len(fict_prices)\n",
    "    \n",
    "    with open('ans1', 'wb') as fp:\n",
    "\n",
    "               pickle.dump(result, fp)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.45550000000001"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Modify `bank_data()` so that it extracts the total consolidated assets (\"Consol\n",
    "Assets\") for JPMorgan Chase, Bank of America, and Wells Fargo recorded each December from\n",
    "2004 to the present. Return a list of lists where each list contains the assets of each bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_data():\n",
    "    \"\"\"Crawl through the Federal Reserve site and extract bank data.\"\"\"\n",
    "    # Compile regular expressions for finding certain tags.\n",
    "    link_finder = re.compile(r\"December 31, (?!2003)\")\n",
    "    chase_bank_finder = re.compile(r\"^JPMORGAN CHASE BK\")\n",
    "    BofA_finder = re.compile(r\"^BANK OF AMER\")\n",
    "    wells_fargo_finder = re.compile(r\"WELLS FARGO\")\n",
    "\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.federalreserve.gov/releases/lbr/\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    link_tags = base_soup.find_all(name='a', href=True, string=link_finder)\n",
    "    pages = [base_url + tag.attrs[\"href\"] for tag in link_tags]\n",
    "\n",
    "    # Crawl through the individual pages and record the data.\n",
    "    chase_assets = []\n",
    "    BofA_assets = []\n",
    "    wf_assets = []\n",
    "    for page in pages:\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "\n",
    "        # Find the tag corresponding to the banks' consolidated assets.\n",
    "        chase_temp_tag = soup.find(name=\"td\", string=chase_bank_finder)\n",
    "        BofA_temp_tag = soup.find(name=\"td\", string=BofA_finder)\n",
    "        wf_temp_tag = soup.find(name=\"td\", string=wells_fargo_finder)\n",
    "\n",
    "        for _ in range(10):\n",
    "            chase_temp_tag = chase_temp_tag.next_sibling\n",
    "            BofA_temp_tag = BofA_temp_tag.next_sibling\n",
    "            wf_temp_tag = wf_temp_tag.next_sibling\n",
    "            \n",
    "        # Extract the data, removing commas.\n",
    "        chase_assets.append(int(chase_temp_tag.string.replace(',', '')))\n",
    "        BofA_assets.append(int(BofA_temp_tag.string.replace(',', '')))\n",
    "        wf_assets.append(int(wf_temp_tag.string.replace(',', '')))\n",
    "        \n",
    "    result = [chase_assets, BofA_assets, wf_assets]\n",
    "        \n",
    "    with open('ans2', 'wb') as fp:\n",
    "               pickle.dump(result, fp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2218960,\n",
       "  2140778,\n",
       "  2082803,\n",
       "  1914658,\n",
       "  2074952,\n",
       "  1945467,\n",
       "  1896773,\n",
       "  1811678,\n",
       "  1631621,\n",
       "  1627684,\n",
       "  1746242,\n",
       "  1318888,\n",
       "  1179390,\n",
       "  1013985,\n",
       "  967365],\n",
       " [1782639,\n",
       "  1751524,\n",
       "  1677490,\n",
       "  1639305,\n",
       "  1574093,\n",
       "  1433716,\n",
       "  1474077,\n",
       "  1451969,\n",
       "  1482278,\n",
       "  1465221,\n",
       "  1471631,\n",
       "  1312794,\n",
       "  1196124,\n",
       "  1082243,\n",
       "  771619],\n",
       " [1689351,\n",
       "  1747354,\n",
       "  1727235,\n",
       "  1610580,\n",
       "  1532784,\n",
       "  1373600,\n",
       "  1266125,\n",
       "  1161490,\n",
       "  1102278,\n",
       "  608778,\n",
       "  635476,\n",
       "  467861,\n",
       "  398671,\n",
       "  403258,\n",
       "  366256]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "The Basketball Reference website at `https://www.basketball-reference.com`\n",
    "contains data on NBA athletes, including which player led different categories for each season.\n",
    "For the past ten seasons, identify which player had the most season points and find how many\n",
    "points they scored during that season. Return a list of triples consisting of the season, the\n",
    "player, and the points scored, (\"season year\", \"player name\", points scored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob3():\n",
    "    '''The Basketball Reference website at \n",
    "    https://www.basketball-reference.com} hosts data on NBA athletes, \n",
    "    including which player led different categories.\n",
    "    For the past ten years, identify which player had the most season points.\n",
    "    Return a list of triples, (\"season year\", \"player name\", points scored).\n",
    "    '''\n",
    "    \n",
    "    TABLE_ELEM_ID = 'totals_stats'\n",
    "    names  = []\n",
    "    points = []\n",
    "    years = range(2010, 2020)\n",
    "    \n",
    "    for year in years:\n",
    "        url = f'https://www.basketball-reference.com/leagues/NBA_{year}_totals.html#totals_stats::pts'\n",
    "        html = requests.get(url).text\n",
    "        df = pd.read_html(html)[0]\n",
    "        \n",
    "        df = df[df.apply(lambda row: row['PTS'].isnumeric(), axis=1)]\n",
    "        df['PTS'] = df['PTS'].astype(int)\n",
    "        df = df.sort_values('PTS', ascending=False).reset_index(drop=True)\n",
    "        names.append(df[\"Player\"][0])\n",
    "        points.append(df[\"PTS\"][0])\n",
    "        \n",
    "    result = [(years[i], names[i], int(points[i])) for i in range(len(names))]\n",
    "        \n",
    "    with open('ans3', 'wb') as fp:\n",
    "               pickle.dump(result, fp)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prob3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "The website IMDB contains a variety of information on movies. Specifically,\n",
    "information on the top 10 box offce movies of the week can be found at `https://www.imdb.\n",
    "com/chart/boxoffice`. Using `BeautifulSoup`, `Selenium`, or both, return a list of the top 10\n",
    "movies of the week and order the list according to the total grossing of the movies, from most\n",
    "money to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob4():\n",
    "    \"\"\"\n",
    "    Sort the Top 10 movies of the week by Total Grossing, taken from \n",
    "    https://www.imdb.com/chart/boxoffice?ref_=nv_ch_cht.\n",
    "\n",
    "    Returns:\n",
    "        titles (list): Top 10 movies of the week sorted by total grossing\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://www.imdb.com/chart/boxoffice\"\n",
    "    html = requests.get(url).text\n",
    "    df = pd.read_html(html)[0]\n",
    "    \n",
    "    df[\"Gross\"] = df[\"Gross\"].replace('[\\$M]', '', regex=True).astype(float)\n",
    "    df = df.sort_values(\"Gross\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "#     df = df[df.apply(lambda row: row['Gross'].isnumeric(), axis=1)]\n",
    "#     df['Gross'] = df['Gross'].astype(int)\n",
    "#     df = df.sort_values('Gross', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    result = list(df.Title)\n",
    "\n",
    "    with open('ans4', 'wb') as fp:\n",
    "               pickle.dump(result, fp)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Lion King',\n",
       " 'It Chapter Two',\n",
       " 'Hustlers',\n",
       " 'Good Boys',\n",
       " 'Angel Has Fallen',\n",
       " 'Downton Abbey',\n",
       " 'Ad Astra',\n",
       " 'Rambo: Last Blood',\n",
       " 'Abominable',\n",
       " 'Judy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "The arXiv (pronounced \"archive\") is an online repository of scientific publications,\n",
    "hosted by Cornell University. Write a function that accepts a string to serve as a search\n",
    "query defaulting to linkedin. Use `Selenium` to enter the query into the search bar of `https:\n",
    "//arxiv.org` and press Enter. The resulting page has up to 50 links to the PDFs of technical\n",
    "papers that match the query. Gather these URLs, then continue to the next page (if there are\n",
    "more results) and continue gathering links until obtaining at most 150 URLs. Return the list\n",
    "of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob5(search_query):\n",
    "    \"\"\"Use Selenium to enter the given search query into the search bar of\n",
    "    https://arxiv.org and press Enter. The resulting page has up to 25 links\n",
    "    to the PDFs of technical papers that match the query. Gather these URLs,\n",
    "    then continue to the next page (if there are more results) and continue\n",
    "    gathering links until obtaining at most 100 URLs. Return the list of URLs.\n",
    "\n",
    "    Returns:\n",
    "        (list): Up to 100 URLs that lead directly to PDFs on arXiv.\n",
    "    \"\"\"\n",
    "    # install chrome driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    \n",
    "    links = []\n",
    "    # open chrome driver\n",
    "#     driver = webdriver.Chrome()\n",
    "    driver.get(\"https://arxiv.org\")\n",
    "    try:\n",
    "        \n",
    "        # Query search bar\n",
    "        input_text = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"query\"))\n",
    "        )\n",
    "        input_text.send_keys(search_query)\n",
    "        \n",
    "        # Enter Search Query\n",
    "        search_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"header\"]/div[2]/form/div/button'))\n",
    "        )\n",
    "        search_button.click()\n",
    "        \n",
    "        # get html\n",
    "        html_text = driver.page_source\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # only find up to 150 links\n",
    "            if len(links) > 150:\n",
    "                break\n",
    "                \n",
    "            # parse links\n",
    "            soup = BeautifulSoup(html_text)\n",
    "            anchors = soup.find_all('a', text=re.compile('pdf'))\n",
    "            links += [ a['href'] for a in anchors ]\n",
    "        \n",
    "            # try to click the next button -- stop if fails\n",
    "            try:\n",
    "                button = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/main/div[2]/nav[1]/a[2]'))\n",
    "                )\n",
    "                button.click()\n",
    "                time.sleep(5)\n",
    "            except Exception:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        time.sleep(5)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "    # return 150 links\n",
    "    return links[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for win32 chromedriver:77.0.3865.40 in cache\n",
      "Driver found in C:\\Users\\Kameron Lightheart\\.wdm\\chromedriver\\77.0.3865.40\\win32/chromedriver.exe\n"
     ]
    }
   ],
   "source": [
    "result = prob5(\"linkedin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ans5', 'wb') as fp:\n",
    "               pickle.dump(result, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def self_check():\n",
    "\n",
    "    # Problem 1\n",
    "    with open('ans1', 'rb') as fp:\n",
    "        res = pickle.load(fp)\n",
    "    assert type(res) == float\n",
    "\n",
    "    # Problem2\n",
    "    \"\"\"\n",
    "        extract the assets for \n",
    "        JPMORGAN CHASE BK NA\n",
    "        BANK OF AMER NA\n",
    "        WELLS FARGO BK NA\n",
    "    \"\"\"\n",
    "    with open('ans2', 'rb') as fp:\n",
    "        res = pickle.load(fp)\n",
    "    assert type(res) == list\n",
    "    assert len(res) == 3\n",
    "    assert type(res[0]) == list\n",
    "    assert len(res[0]) == 15\n",
    "    assert type(res[0][0]) == int\n",
    "\n",
    "    # Problem 3\n",
    "    \"\"\" \n",
    "    Â  Â  make sure your first tuple in the list \n",
    "    Â  Â  aka res[0] corresponds to the season that ended in 2019, \n",
    "    Â  Â  I won't be checking how you stored the year I'll just be \n",
    "    Â  Â  expecting them to be in order with the last entry being \n",
    "    Â  Â  the season that ended in 2010 \n",
    "    \"\"\"\n",
    "    with open('ans3', 'rb') as fp:\n",
    "        res = pickle.load(fp)\n",
    "    assert type(res) == list\n",
    "    assert len(res) == 10\n",
    "    assert type(res[0]) == tuple\n",
    "    assert len(res[0]) == 3\n",
    "    assert type(res[0][1]) == str    #player's name\n",
    "    assert type(res[0][2]) == int    #points scored\n",
    "\n",
    "\n",
    "    #Problem 4\n",
    "    \"\"\" \n",
    "    Â  Â  the first movie in the list should be the movie with \n",
    "    Â  Â  the most money or that grossed the most money, \n",
    "    Â  Â  not sure how to say it \n",
    "    \"\"\"\n",
    "    with open('ans4', 'rb') as fp:\n",
    "        res = pickle.load(fp)\n",
    "    assert type(res) == list\n",
    "    assert len(res) == 10\n",
    "    assert (type(res[0]) == str or type(res[0]) == np.str_)\n",
    "\n",
    "\n",
    "    #Problem 5\n",
    "    \"\"\" \n",
    "    Â  Â  for this problem store the list that you get when \n",
    "    Â  Â  the default 'linkedin' search query is passed in. the first \n",
    "    Â  Â  element in your list should beÂ 'https://arxiv.org/pdf/1907.12549' \n",
    "    \"\"\"\n",
    "    with open('ans5', 'rb') as fp:\n",
    "        res = pickle.load(fp)\n",
    "    assert type(res) == list\n",
    "    assert len(res) > 100\n",
    "    assert len(res) < 150\n",
    "    assert type(res[0]) == str\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
